
\documentclass{article}
\usepackage[nottoc,numbib]{tocbibind}
%\usepackage[nottoc]{tocbibind}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[table,xcdraw]{xcolor}
\usepackage{listings}
\usepackage{multirow}
\usepackage{blindtext}
\usepackage{tikz}
\usepackage{float}
\usepackage{fontspec}
\usepackage[
	margin=1.5in,
	footskip=1.2in
	]{geometry}
\usepackage{fancyhdr}
\usepackage{mwe}
\usepackage{graphbox}
\pagestyle{fancy}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{xcolor}
\lstset {
	breaklines=true
}
\usepackage{tocloft}
%JAMK custom color, used to print The Line
\definecolor{JamkBlue}{rgb}{0,90,125}
\renewcommand\cftsecleader{\cftdotfill{\cftdotsep}}

%Title page needs special attention: line in the left
\fancypagestyle{TitlePage}
{
	\fancyhf{}
	\renewcommand{\headrulewidth}{0pt}	
	\lhead{
		\begin{tabular}[t]{l@{\hspace{3cm}}l}	
		\includegraphics[scale=0.22]{jamk-logo}
		\end{tabular}	
		}
	\newgeometry{left=4cm}	
	\cfoot{\includegraphics[valign=B,scale=1.2]{jamk-footer} }	
}

%clear headers&footers
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
%insert jamk -footer logo to each page
%\cfoot{\includegraphics[valign=B,scale=1.2]{jamk-footer} }
\setmainfont{Carlito-Regular}
\parskip = \baselineskip
\usepackage[parfill]{parskip}
 
\begin{document}
\onehalfspacing
\begin{titlepage}
\thispagestyle{TitlePage}
    \vfill
    \begin{tabular}[t]{l@{\hspace{0cm}}l}
     &
     \begin{tabular}[t]{p{0.85\textwidth}}
     \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 
     \textbf{\Large {How to self-assess and audit application deployment in public cloud? }} \\ [2cm]
	\large{Pinja Koskinen} \\	
	\large{Vesa Simola} \\
       { \date{ } }  
	\\ \\ \\ \\ \\ \\ \\ \\ \\
	\large{Masters thesis}\\
	\large{XXXXX 2018}\\
	\large{Cyber security}\\
	\large{Master's degree programme in cyber security}
	\end{tabular}
    \end{tabular}

\end{titlepage}

\clearpage 
\begin{table}[]
\centering
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Author(s)\\ Pinja Koskinen, Simola, Vesa\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Type of publication\\ Masters thesis\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Date\\ Month Year\end{tabular}                  \\ \cline{3-3} 
                                                                                        &                                                                                                & \begin{tabular}[c]{@{}l@{}}Language of publication:\\ English\end{tabular} \\ \cline{2-3} 
                                                                                       & Number of pages                                                                                & Permission for web publication: x                                          \\ \hline
\multicolumn{3}{|l|}{\begin{tabular}[c]{@{}l@{}}Title of publication\\ Title\\ possible subtitle\end{tabular}}                                                                                                                                                       \\ \hline
\multicolumn{3}{|l|}{Degree programme}                                                                                                                                                                                                                                \\ \hline
\multicolumn{3}{|l|}{\begin{tabular}[c]{@{}l@{}}Supervisor(s)\\ Last name, First name\end{tabular}}                                                                                                                                                                  \\ \hline
\multicolumn{3}{|l|}{Assigned by}                                                                                                                                                                                                                                    \\ \hline
\multicolumn{3}{|l|}{\multirow{5}{*}{Abstract}}                                                                                                                                                                                                                      \\
\multicolumn{3}{|l|}{}                                                                                                                                                                                                                                               \\
\multicolumn{3}{|l|}{}                                                                                                                                                                                                                                               \\
\multicolumn{3}{|l|}{}                                                                                                                                                                                                                                               \\
\multicolumn{3}{|l|}{}                                                                                                                                                                                                                                               \\ \hline
\end{tabular}
\end{table}
\clearpage

\doublespacing
\tableofcontents
%Insert page number to right upper corner (header)
\pagebreak 
\setcounter{page}{1}
\rhead{\thepage}
\section{Introduction}
\subsection{Background of the study}
The cloud sign in network and software diagrams has been used for many years to indicate and abstract myriad of details concerning message flows, protocols and communications across a network. This abstraction has since evolved to include computing, storage and applications, both virtual and physical. New flexible cloud capabilities are emerging regularly, better yet at lower costs using pay-per-use models. With these new developments comes the increased security, privacy and IT-governance challenges. (Kris Jamsa, 2012)
\par
One of the key aspects in this thesis is the concept of security in cloud context, security being defined as a process of maintaining sufficient level of perceived risk.(Bejtlich,2004).
In their paper "Cloud computing and security" Xingming Sun,Zhaoqing Pan and Elisa Bertino gave us the following definition of cloud computing that gives another insight to the meaning of cloud computing and the inherit security aspects therein: Cloud computing is generally built from hardware and software components residing in one or more data centers within a single organization and used for sharing resources of those data centers amongst several customers or services. To put it another way, cloud computing is similar to large pool of resources that are abstracted and virtualized to provide computing, storage, applications and services that are delivered from the shared pool. Given that the pooling of resources is so concentrated and the architecture to deliver this service is so complex it is a given that there are several things that need to be investigated from technological and management perspectives when it comes to cloud computing security. Examples of these areas to investigate include security architecture model, data security, cloud computing encryption, privacy protection, access control and authentication, virtualization security and others such as customer isolation and cross-domain service security.(Xingming Sun,Zhaoqing Pan and Elisa Bertino, 2018)
\par 
There is seemingly some consensus that by placing all customer data and services in cloud would mean that successful attacker could gain access to large quantities of confidential information, meaning that the risk of data loss of security breach is higher in cloud than in traditional enterprise data centers. This idea has its basis in the thinking that the cloud service provider and its infrastructure is more lucrative target than traditional data center. On the other hand, there is consensus that cloud can be more secure than traditional data center. Reasoning being that it is easier to protect larger quantities of services in fewer locations and it would also be easier to use the latest technologies of protection in centralized manner. Also, given that costs tend to increase as more security is implemented , economics of scale, more consistent deployments, centralized log management and such consolidated measures help to reduce the cost of security compared to legacy server farms. (Bond, 2018). To add insult to injury, statements such as "the cloud is risky" do not deliver any useful security information. Instead,these kind of statements should include probabilities and measured impacts to give one any value when making decisions.(Raymond Pompon, 2016) 
This controversy of cloud computing is further illustrated by Tim Mather, Subra Kumaraswamy and Shahed Latif in their book Cloud security and privacy. They do it using the familiar "mind the gap" -sign seen (and heard) in the London subway. Principle is that while we constantly hear the choir of "cloud computing good" we also get to hear the "could security bad" verse. Mind the gap meaning that one has to watch his step. Yet it is apparently not clear what is wrong with the cloud security.(Tim Mather, Subra Kumaraswamy and Shahed Latif 2009).
\par
Aforementioned problem landscape is further fuzzed by the fact that each industry has its particular characteristics and risks. This means that the level of tolerance for risks can differ significantly from one field of industry to another, example of this could be a banking sector that is very concerned with the exposing their records in the cloud. Some other industry might decide that the benefits of cloud outweigh the potential risks and this might mean that they are more forthcoming towards cloud. This means that the security needs to be viewed as relative to what customer has at the moment and on the other hand, what the cloud service provider can produce. (Ryan Ko, Raymond Choo, 2015)
\par
This thesis tries to find answers to what some of that might mean and what should be taken into an account when evaluating ones cloud posture.
\subsection{Objective of the study: creating criteria-based self-assessment}
Criteria-referenced self-assessment is a concept where individual or organization gather information about their abilities or progress, then compare that data to explicitly defined criteria or standards and then amend or improve their practices and understanding of the topics based on the results.
Purpose of the self-assessment is to detect areas where organization or individual is strong and on the other hand, to find weaknesses to improve on. It is stated that feedback plays crucial role in learning. (Heidi Andrade, Anna Valtcheva, 2009)
The lack of feedback in education environment is largely due to fact that few teachers have the resources to regularly respond to the work done by students. Luckily, research shows that pupils themselves can be effective origins of feedback via means of self-assessment.(Heidi Andrade, Anna Valtcheva, 2009).
\par
There is some research suggesting that just by exposing the students to rubric may improve students insight on the subject matter and also to increase the quality of their work. But even better results can be gained by actively engaging the student to utilize the rubric to self-assess their work. (Heidi Andrade, Anna Valtcheva, 2009). Similarly to this, we hope that this self-assessment criteria created in this theses would be usable and approachable enough to be similarly useful. 
So, the ultimate goal of the study was to create self-assessment questionnaire for cloud computing security. Motivation for this is that, similarly to the feedback scenario in education, currently there is not all that much regulation to reflect on that addresses the cloud specific issues and risks, likely this will take time for the standards to adjust. (Halpert, 2011). This is also true when it comes to the widely accepted and adopted standards and guidelines currently available for cloud services. (Ryan Ko, Raymond Choo, 2015)
\subsection{Methods of the study}
By answering to a question such as "what kind of information we're trying to come up with this research?" we can come up with a reasonable research method (Vilkka 2015).
The answer to that question and hence the goal of this thesis was to come up with a self-assessment criteria to evaluate ones cloud posture in terms of security.
To this end, research was done by first doing qualitative research utilizing literature available to us from the Finnish theseus service (Arene ry - the Rectors' Conference of Finnish Universities of Applied Sciences), EPFL library (École polytechnique fédérale de Lausanne, BEAST collection) and the Finna service (free access to material from Finnish museums, libraries and archives) the writer of this thesis, and on the other hand the topics other researchers had found noteworthy, this was done to come up with reasonable background information to use as a basis for the self-assessment. We also reviewed IETF rfcs, mainly from the standards track in addition to some white papers produced by commercial entities. White papers where mostly select based on completely subjective understanding of the credibility and mostly used as a filling material on topics where not enough literature was found using reasonable effort. Search terms used where: cloud security, change management, network security, log management, data hiding, key management, access management, security standards, life cycle management, privacy management, data governance. In addition to those, we did use search terms such nuclear power plant security to find documentation that was not directly related to cloud security but more specific looks to other fields of security.
\par
Practical part of the research was to actually implement the self-assessment questionnaire that is handled as an separate attachment of this thesis. This assessment is based ont he findings of the above literature review.
\section{General overview of cloud}
Cloud service is generally understood as a product that consists of services hosted in the Internet. This could include servers, networks, storage systems, software applications and other services. These products could be running from anywhere in the world, in a distributed manner. Cloud allows users to utilize applications without modifications or access to their locally available files and services can be reachable from any location within the Internet. Also, in some cases users may share files, data and information between several systems and other users via the cloud infrastructure.
(Suikkanen, 2013 s8)
\par
To name a few higher level motivators that might push companies towards cloud, let us consider the following (Tim Mather, Subra Kumaraswamy, Shahed Latif 2009):
\begin{description}
        \item[$\bullet$ Initial investment] is more manageable than buying complete set of infrastructure.
        \item[$\bullet$ Economies of scale] provided to the cloud service provider help to keep costs and delivery times down.
        \item[$\bullet$ Open standards] by open source software are acting as the foundation of the cloud solution.
        \item[$\bullet$ Sustainability] via means of service provider having already done the major capital investments.
\end{description}
All of the above are beneficial elements of the different categories of different cloud categories.
Cloud community uses the following models to categorize their services: Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS). (Ahlgren 2012, s7)
Cloud hosting can be done in few different manners: Private cloud, public cloud, hybrid cloud and community cloud. (Suikkanen, 2013).
Aforementioned deployment and hosting models are discussed next.
\subsection{Cloud hosting types}
As a concept, cloud computing can have multiple hosting types that differ from each other, these can be seen as ways of delivering the computing service. We will next describe some of the different characteristics of these hosting types as they do impact their ideal use cases.
\subsubsection{Public cloud}
Public cloud is hosted on the service provider facilities and all maintenance, modifications and upgrades are done by the service provider, meaning that the customer has no control over the hosted infrastructure. One exception to this is the fact that certain service providers give customer the options of choosing from several geographical locations from which to run their service. (Juha Ahlgren 2012, s12).
The economy of scale can mean that the public cloud can offer efficient storage, compute and connectivity at reasonable price. This can be especially true with the charging models where customers are required to pay only for the service they require and use. (Saara Suikka, 2013, s11)
\subsubsection{Private cloud}
Private cloud is understood as a service that is being operated by a service provider as a service to be used by single customer. Private cloud tends to use the same techniques as public cloud but they are configured to help the customer organization be more responsive and efficient in the IT resource usage than with traditional IT operation model. (Saara Suikkanen, 2013 s11)
There are generally two types of private clouds, ones that are hosted on the customer premises and then there are those that are hosted on service provider infrastructure. It should be noted that while cloud infrastructure could be externally hosted, it is still considered a private cloud if the infrastructure is solely used by single customer organization. (Juha Ahlgren 2012, s10).
Infrastructure on public cloud on the other hand is shared among the various customers of a service provider. (TAMOU, Aikaterini, 2014, s6).
\subsubsection{Hybrid cloud}
Combinations of the public and private cloud are called hybrid clouds. These clouds can tie the infrastructures of a private and public cloud together and allow the customer to extend their capacity beyond what is available in the private cloud by additionally utilizing the public cloud on time of need. This is called cloud bursting. Meaning that customer uses private cloud under normal circumstances but during peak load some or all parts of the service can be transported to public cloud.(Juha Ahlgren 2012, s13)
\subsubsection{Community cloud}
Fourth and final form of cloud is the community cloud. Community cloud is a multi tenant cloud setup that is utilized by several organizations that may share a common interest or computing concerns. Such concern could come in a form of a compliance requirement, audit requirement or that the organizations require high speed access to common data, for example research organizations working on a common project. (Saara Suikkanen 2013, s12)
\subsection{Cloud deployment models}
Cloud deployment models have significant strengths and weaknesses across the three different deployment modes. From commercial standpoint they these models provide greater flexibility and they try to make IT more accessible to more consumers. (Winkler, 2011) Following is a rough characteristics of each of the three models.
\subsubsection{Infrastructure as a service}
Infrastructure as a service is the most basic service in the cloud landscape, it generally means an offering consisting of infrastructure, physical or virtual machines and other related resources like storage of images, networking and security features such as firewalls and load balancers and bundles of software. (Saara Suikkanen, 2013 s13)
The benefit of the IaaS cloud for the customer is that certain data center related activities can be abstracted and used from for example a web interface or an API. There is no need to manage all levels of the infrastructure anymore and administrative tasks can mostly focus on server side level like operating systems management and maintenance and third party software maintenance. (Kavis, Michael. "Infrastructure as a Service". Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS). John Wiley \& Sons. 2014. Books24x7. <http://common.books24x7.com.ezproxy.jamk.fi:2048/toc.aspx?bookid=62597> (accessed September 25, 2018), Infrastructure as a Service ).
As in this type of a cloud service only the infrastructure is provided, all software related development and administration responsibilities are left to the customer (Kavis, Michael. "Infrastructure as a Service". Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS). John Wiley \& Sons. © 2014. Books24x7. <http://common.books24x7.com.ezproxy.jamk.fi:2048/toc.aspx?bookid=62597> (accessed September 25, 2018), Infrastructure as a Service ).
So, it is worth to emphasize that while customer has limited or no control of underlying architecture that is used to provision the cloud based services, customer is still responsible for proper use and care of the cloud resources, for example the configuration of an application. (STAMOU, Aikaterini, 2014,s5)
\subsubsection{Platform as a service}
As stated above IaaS does not address the various scalability issues or automation challenges faced by organizations especially from the perspective of a software. All the parts of the software infrastructure must be provided by the customer. To ease this task PaaS providers can provide software platforms to certain level. Typical software platforms can be for example databases, logging and payments services, which can be used via various APIs (Kavis, Michael. "Platform as a Service". Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS). John Wiley \& Sons. © 2014. Books24x7. <http://common.books24x7.com.ezproxy.jamk.fi:2048/toc.aspx?bookid=62597> (accessed September 25, 2018) )
Several PaaS related technologies also aim at automating the provisioning procedures for the virtual machines and containers that actually run the application. Examples of these services could be for example a Kubernetes platform, which would provide an API for containers for automatic scalability. Containers are relatively new concept in computing but they are used to package the application and its dependencies in to a manageable units for distribution and running in cloud platform. (What is a container? Docker documentation 2018). These containers can then be housed in orchestration tools such as the aforementioned Kubernetes or Docker swarm.
As a conclusion, PaaS deployment could be considered being one level above the Software as a Service deployment as it eliminated the need for customer owned infrastructure for the deployment of a software application. (Saara Suikkanen 2013, s14)
\subsubsection{Software as a service}
Software as a service is a method of delivering software application from cloud via Internet connectivity with the least amount of manual work from the customer. Using SaaS only requires configuration and user management from the customer, leaving everything else for the service provider. The advantages to the customer is the lack of need to maintain the platform and not needing any personnel to execute the maintenance tasks, which is beneficial especially when talking about services that do not belong to the core functionalities of the customer. (Kavis, Michael. "Software as a Service". Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS). John Wiley \& Sons. © 2014. Books24x7. <http://common.books24x7.com.ezproxy.jamk.fi:2048/toc.aspx?bookid=62597> (accessed September 25, 2018) ) Naturally, the SaaS services can not be used for software that require any heavier tailoring than just predefined configuration changes.
A real-life example that illustrates the stacking of cloud services and the SaaS could be a email service that has its customer specific front ends running in containers on service provider orchestration tool that utilizes virtual machines housed in service provider facilities and hypervisors somewhere. SaaS services are nowadays very common (Kavis, Michael. "Software as a Service". Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS). John Wiley \& Sons. © 2014. Books24x7. <http://common.books24x7.com.ezproxy.jamk.fi:2048/toc.aspx?bookid=62597> (accessed September 25, 2018)).
Key point to understand the SaaS model is that customer has no control of the underlying software deployment or the computing infrastructure in SaaS model. (STAMOU, Aikaterini, 2014,s5). This is essential differentiator between SaaS and PaaS.
\section{Security in cloud}
As with any environment business continuity planning and disaster recovery planning apply, regardless if the service is ran on-premisses or in cloud. (Halpert, 2011).
Hence, we'll start this chapter by describing business continuity plan and disaster recovery plan before diving further into the recognized risks.
\subsection{Definition and importance of business continuity  and disaster recovery plan}
Business continuity plan is clear plan that aims to ensure that critical functions of a given organization are capable to operate in case of a disaster.
Business continuity plan should identity the essential resources such as personnel, systems and infrastructure that is required to run the essential emergency business operation and how to later on reestablish all the business functions. (Childs, 2008)
Disaster recovery plan is usually coupled with the business continuity plan, but it is aimed more towards how to deal with the immediate crisis to safeguard the personnel and also to limit further damage to equipment.(Childs, 2008)
\subsubsection{RTO - recovery time objective}
RTO roughly translates to how quickly customer needs to recover in case of disaster taking place. RTO has direct impact on the budget and resourcing required to recovery operations. As an example, if customer is to assume RTO of 3 hours, it is essential to invest hefty amount of money on a recovery site and make sure that it is operational with in the three hour window. If in turn, customer is expecting three week RTO service provider could, in some cases, just simply wait for the repairs in data center to take place. (Vora,2017)
\subsubsection{RPO - recovery point objective}
When RTO is mostly about the time we have available before operations must continue, RPO is translated into to the amount of data that is acceptable to loose in case of disaster. RPO can give indications as to how robust infrastructure is required to run the service. Example of this would be RPO of five hours, meaning that backups of the service must be taken every five hours. This is to keep the amount of "in flight" data at bay. (Vora,2017)
\subsection{Common cloud security aspects}
As cloud is a relatively new approach to computing it is no wonder there is some uncertainty about how security at its various levels can be achieved. This uncertainty has led to decision makers to state that security is their primary concern with cloud computing. (Tim Mather, Subra Kumaraswamy, Shahed Latif, 2009). 
Some general level challenges of cloud computing are identified as follows by Ben Halpert in his book Auditing Cloud Computing: A Security and Privacy Guide.
\begin{description}
	\item[$\bullet$ Availability] can be at risk as customers might consume more of the shared resources than expected. This is especially true in public cloud.
	\item[$\bullet$ Vast resources] of the cloud could be used to launch denial of service attacks.
	\item[$\bullet$ Data residency] is a factor as different countries and regions have different requirements for information handling.
	\item[$\bullet$ Multi tenancy] is what allows the economics of scale, it is also a compliance consideration when same infrastructure is shared amongst customers.
	\item[$\bullet$ Log management] of shared infrastructure might present an issue as information from multiple tenants could be visible in the same log files. 
	\item[$\bullet$ Performance] and service levels of the cloud are based on the services purchased, these metrics can be controlled by service level agreements.
	\item[$\bullet$ Data evacuation] process should be addressed as it sets the boundaries how information is removed from shared infrastructure.
	\item[$\bullet$ Supervisory access] is of interest as service provider has the highest level of access to the infrastructure.
\end{description}
Some of the more detailed security concerns can be seen as shared among all the deployments while others are more tied to specific deployment model.
Tim Mather, Subra Kumaraswamy and Shahed Latif describe the following barriers for cloud implementations that are shared amongst the deployment models.
\begin{description}
        \item[$\bullet$ Privacy] is essential and it may not be obvious if the cloud model meets the current and upcoming requirements to safeguard privacy.
        \item[$\bullet$ Connectivity] is mandatory to reach the service. High speed and reliability are critical for the user experience.
        \item[$\bullet$ Reliability] requirements are high as enterprise applications are expected to be available 24/7.
        \item[$\bullet$ Interoperability] with traditional non-cloud software is not given.
        \item[$\bullet$ Reliance on the service provider] and vendor lock-in are threats that need to be addressed on contract level.
        \item[$\bullet$ Economic value] can be at risk due to hidden costs that are not obvious. It should be also noted that transitioning to cloud is not free.
        \item[$\bullet$ IT governance] still has to be taken into account to make sure that the cloud deployment is in line with the business needs.
        \item[$\bullet$ Political and global boundaries] can be factors when considering if it is all right to store for example customer data to outsourced data center.
        \item[$\bullet$ Changes in IT organization] has to have the skills needed to operate the cloud environment and on the other hand IT organizations role might change due to a major cloud deployment.
\end{description}
Given the suggested flexibility of the cloud deployments and the vast number of threats shown above it is only natural that from an IT manager's perspective the very nature of the cloud architecture bypasses and fights against the well-known tools and frameworks of security.
This is illustrated by the ease (and contradiction therein) in which services can be migrated, created and deployed in a cloud environment, but this does not remove the need for compliance and security. (Raghu Yeluri, Enrique Castro-Leon 2014). 
Next, we'll discuss some of the security concerns of different cloud deployment models starting from the more general ones towards more deployment model specific ones.
\subsubsection{Vendor lock-in}
Ryan Ko and Raymond Choo give a the following analogue to vendor lock-in in their boot "The Cloud Security Ecosystem" from 2015: The concern of vendor lock-in is often described as the “Hotel-California” syndrome. You can check-in but you can never leave. (Ryan Ko,Raymond Choo 2015, chapter 5.1).
Essentially this means that a service provider produces the service using their own standards, protocols and policies, leading to a situation where customer is effectively tied to their current service provider. Meaning that the customer cannot take their business elsewhere, or the costs of doing that would become too high. There could be several reasons why customer might want to migrate away from a given vendor, for example unacceptable increase in the costs at the time when it is time to renew the contract or when the service provider ceases to operate as a business. In order to avoid vendor lock-in customer should review the service level agreement, ask the service provider what is their policy on data moving and how this effects the support available if customer was to migrate to another service provider. Customer should also try to select technologies that are available on multiple service providers. Customer could also make sure that they can have the copy of the data their own premise in a openly available raw format. Customer should also confirm that the customers application does not need to be written in exotic proprietary language in favor of openly available ones, such as C, Java or Python. (Ryan Ko,Raymond Choo 2015)
\subsubsection{Requirements set by regulation}
Combining the relative freshness of cloud as a concept and that there are many service providers to choose from it is unfortunate that there is not all that much rules and guidelines for cloud implementations.
It is likely that in the future there will be more regulation for cloud services but it is hard to predict what the impact of the regulation will be. On the other hand, this new regulation might make it easier for customers to select service providers, but downside to this is that it could also lead to a situation where the cost benefits of cloud would shrink as customer would likely have to pay the bill of implementing the requirements defined by regulators. (Halpert, 2011)
\par
To better understand the regulatory aspects in cloud computing we first need to define what is meant by regulation.
The dictionary definition of regulation states that regulation is a rule or directive made and maintained by an authority (Oxford dictionary, 2018).
To broaden the meaning we can look at Ben Halperts book Auditing Cloud Computing: A Security and Privacy Guide from 2011 where he gives the following, quite descriptive, definition: A regulation is a rule or law, hence there should be consequence for not abiding, and that there shall be policing in a form of compliance. Basic idea of regulation is broadly protective, for example to protect assets such as stake holder value or citizens. (Halpert, 2011)
\par
Halpert goes on giving us few international examples of regulation:
\begin{description}
	\item[$\bullet$ Federal Information Security Management Act] is legislation that aims to improve all aspects of system security for federal agencies of the United States.
	\item[$\bullet$ Sarbanes-Oxley Law] is legislation for publicly traded companies and their reporting systems with the idea of increasing transparency and accountability.
	\item[$\bullet$ Privacy Laws] are various privacy specific laws on multiple levels, state, federal and EU.
\end{description}
In Finland we have this thing called Katakri that essentially is a auditing tool for authorities. Katakri can be used to evaluate the capability of organization when it comes to security of information that is classified as confidential. (Katakri, 2015)
Another fine example of regulation that is effecting us in Finland is the EU General data protection regulation, or GDPR for short. GDPR officially states that stronger rules on data protection mean that people have more control over their personal data and that businesses benefit from a leveled playing field. (Official GPDR website, https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules\_en as viewed on Jan 2019)
\par
Now that we have established what regulation is we can think of reasons why regulation exists. Ben Halpert states that regulation could be identified as a counter reaction to the failures of security. As an example he showcases the Sarbanes-Oxley and Enron where authorities determined that Enron had failed at policing itself, in essence processes was defined but not implemented, resulting in damage to shareholders. After this incident Public company accounting oversight board (PCAOB) was formed to create a framework consisting of rules to follow for publicly traded companies.
PCAOB went to create the rules based on at that point best known practices (COSO, Committee of sponsoring organizations of the Treadway Commission) that was already in place. Using these already defined best practices allowed PCAOB to quickly setup the audit criteria and guidelines. This resulted in Sarbanes-Oxley compliance program.
(Halpert, 2011).
\par
This could be summarized so that the regulations appear when there is complexity and possibly high risk, and that regulation should be based on known frameworks and standards in order to provide guidance and compliance programs.
To put regulation in to cloud context the reason for the need of regulation might surface in order to provide fair playing field and to address problems that could be related to harmonizing regulation across national borders.
\subsubsection{Global data residency}
To add insult to injury, it should also be stated that compliance with regulation can be a complex topic in multinational setting as there can be significant overlap amongst legislation and regulation in various countries, sometimes they can even be conflicting. Privacy is one of the most complex and difficult topics within the multinational compliance. Stronger privacy protection takes place in Europe than in United States and regulation is strict concerning what information is deemed as acceptable to collect, where it must be stored, not to mentioned where it may be processed. This is illustrated by EU Council Directive 95/46 that limits the transfer and processing of personal data outside borders of the European union. It is important that the service provider has solid strategy and planning to deal with the regulation and legislation related to this topic. To summarize, it is important as customer to understand the jurisdictions where data can be located and the relevant privacy policies of that area. Also customer should make sure that proper controls and policies are in place to ensure that the privacy issues are not violated.(Ryan Ko, Raymond Choo, 2015)
\subsubsection{Division of responsibility}
Based on the security concerns identified above it is essential to understand the concept of division of responsibility.
Term division of responsibility means that the responsibility of the service and the data therein is shared between the customer and the cloud service provider as defined by Jiafu Wan,Kai Lin,Delu Zeng, Jin Li, Yang Xiang, Xiaofeng Liao, Jiwu Huang and Zheli Liu in their conference paper on SPNCE 2016.
Same paper also clarifies this by stating that this division of work may lead to unexpected consequences and that it may be difficult to clearly define who can be held responsible of what as there are likely multiple factors at play on the same time.
Combining the "many hands working together" -problem with the long list of identified security concerns this is a factor worth considering.
\par
Another aspect to this division of responsibility is pointed out by Donna R. Childs in her book Prepare for the Worst, Plan for the Best: Disaster Preparedness and Recovery for Small Businesses: 
Service providers likely want to tie their customers to the service providers offerings as much as they can. Reasoning being that if customer for whatever reason tries to change their service provider they might find out that they have been locked-in by relying on certain functionalities offered by the service provider. Meaning, that in the end it might not be enough to just change some portion of customers service, but to actually make other far more significant changes. When the disaster strikes it is not a good posture to have ones hands tied like this. One approach to lessening this risk is to make sure that the customer has good lines of communications with the candidate service providers and possibly their management as well. This can be accomplished by taking part in information sessions organized by the service provider as these can be a good opportunity to interface with the senior management of the service provider customer is evaluating. To this end, it is a good idea to tell the service provider that the customer is preparing a contingency plan and that customer would appreciate service providers recommendations.(Childs, 2008). 
\par
Assuming the pre-existing customer-service provider -relationship there could be a need to request more specific information, in these situations there are at least three options. Maybe the most straight forward one being to send the list of questions to the service provider and give them some deadline for answers. This approach relies solely on the service provider to tell the truth in their answers. One approach to make it more tight is to include formal attestation clause at the end for executive to sign. Second, bit more invasive approach is to request the service provider to include additional documentation alongside the answers. These documents could be screen shots, access-control list configurations, outputs of vulnerability scans and so forth, they can be used as additional proof that the controls are implemented. Third, and the most invasive method is to send a team on-site and conduct a straight review in person. This would need to be planned and executed according to agenda, including interviews of roles of interest. This is the most resource intensive option, particularly if the business of service provider is very distant of customers business. It is also possible to hire external consultants and auditors for this kind of review. (Raymond Pompon, 2016). 
\subsubsection{Segregation of duties}
Just like with customers own IT environment, customer should ensure that the service provider is adequately safeguarding against issues related to segregation of duties concerning the cloud service that is being offered to customers.
To definition of this problem with segregation of duties could be described as cases where single user is able to both initiate and approve an action. Sumner Blount and Rob Zanella gave the following example of this in their book "Cloud Security and Governance: Who's on your cloud?" in 2010: As an example accounts payable administer who can both establish a new vendor record, and approve paymets to that very same vendor. They also say that issues with segregation of duties can be challenging to identify and to this end very specific policies are required to prevent these issues taking place. Technology can be used to identify and possibly correct these situations as they take place, all in all, review of the service providers policies, strategy and abilities in this field is important. (Sumner Blount, Rob Zanella, 2010). 
\subsubsection{Importance of incident response}
Despite all the implemented controls and righteous plans and ideas for security and availability, the bad thing will eventually happen. This could include various things such as attempts at attacking the environment, successful attacks on the environment, challenges caused by software issues etc. It is important to make sure that the service provider has a sufficient strategy on incident response to handle these issues. Customer should know the procedures of creating, following and reporting of incidents. Customer should ask questions such as how is the customer notified and what kind of visibility is given to the customer to gain more information on incidents detected by the service provider. Does the service provider have a proper plans to act on a PR disaster, such as loss or leak of credit card information? Possible the single most important question to ask is to verify that the service provider and their plans on incident response are consistent with the plans of the customer? (Sumner Blount, Rob Zanella, 2010)
\par
Its stated that incident response resource should not only be a seen as intrusion detection system to alert on network and host level events, but also computer security incident response team (CSIRT) should be established. CSIRT needs to be able to:
\begin{description}
	\item[$\bullet$ Analyze] notifications of events
	\item[$\bullet$ Respond] to the event if this is required, based on the analysis
	\item[$\bullet$ Escalate] the issue as required and by predefined procedures
	\item[$\bullet$ Reporting] on identification, resolution and post-incident to proper parties
\end{description}
These capabilities should ideally be present not only on the service provider but also on the customer side.(Ronald L. Krutz, Russell Dean Vines, 2010)
\par
NIST Special Publication 800-61, "Computer Security Incident Handling Guide, Recommendations of the National Institute of Standards and Technology" from January 2004 splits the incident life-cycle into four parts:
\begin{description}
	\item[$\bullet$ Preparation]
	\item[$\bullet$ Detection and analysis]
	\item[$\bullet$ Containment, recovery]
	\item[$\bullet$ Post-incident actions]
\end{description}
The above topics highlight few areas that could be worth confirming with the service provider. For example on the topic of preparation, what kind of mechanics the service provider has in place to prevent attacks from succeeding? Does service provider implement regular risk analysis, what kind of patch management and host security scheme they have and what kind of user training and education takes place in matters of security? Detecting successful attack is usually challenging. To this end it might be worthwhile to investigate if the service provider does some sort of profiling of the expected system behavior to understand what is normal? What kind of log management and analysis tools are being used and how? How the detection processes is tied to the communication processes with the correct parties? Assuming that the service provider can detect the attack customer should then investigate their capabilities to contain the threat. For this purpose customer could ask questions such as: What kind of means does the service provider have in order to determine for example the user accounts that might have been compromised, or how will the service provider detect files that might have been changed by the attacker? To gain insight on the service providers capability of post-incident actions customer might ask things such as, how does the service provider report what exactly took place during the attack, or if the service provider has means to learn from the incident that took place? And also, what corrective actions could be taken to prevent similar incidents from taking place in the future, and how these improvements would be communicated? (Ronald L. Krutz, Russell Dean Vines, 2010)
\subsection{Security aspects in public cloud}
Based on what has been written above it is likely that it is taken as a given that in public cloud there are multiple tenants on the same physical infrastructure.
Be that as it may, most public clouds offer software-based separation and permission control to maintain isolation between customers. Hardware level separation might be an option, but with likely additional costs involved. It is essential to understand how the platform-of-choice implements the multi tenancy, for example if it supports the concept of having multiple directory services, such as Microsoft Active Directory or LDAP, one for each tenant.(Bond, 2018).
Responsibility of patching and updating servers in public cloud generally falls to the service provider, but this can also cause unexpected risks to customer systems and applications. Hence, close interaction with the service provider is required to ensure that no new risks are introduced  or availability issues surface due to service provider doing maintenance. (Halpert, 2011).
It should be also noted that the highest level of access to the infrastructure is e.g the supervisor -level access, is held by the services provider. (Halpert, 2011)
\subsection{Security aspects in private cloud}
Unlike with public cloud, multi tenancy is slightly less of an issue in private cloud, in fact private cloud on its own could be seen as an approach to solve the multi tenancy issue. (Bond, 2018)
Ben Halpert highlights that usually both consumer and service provider are internal to the organization, this allows more control over aspects of the cloud service, such as quality of service.
Example of this is that internal can more easily impact the way workload is ran based on its criticality to the business. This control comes at the price of customer paying for the whole infrastructure as it is dedicated.(Halpert, 2011) 
\subsection{Security aspects in hybrid cloud}
As hybrid cloud is stated above being a mixture of both public and private cloud all the same rules apply. It should still be noted that while portions of the service may run in public cloud at times, the same security precautions and metrics should still be met as if the service was running solely in private cloud.
\subsection{Security aspects in Infrastructure as a service}
It is key element to understand that the service provider has means to view the activities of any virtual machine running inside a infrastructure as a service cloud. (Halpert, 2011)
Also, as stated above customer is responsible for implementing the required patching inside virtual machines himself, this is true even while service provider would be patching the hypervisor level.
\par
Another way to describe this is illustrated in the Cloud security and privacy book by Tim Mather, Subra Kumaraswamy and Shahed Latif would be to split the security of infrastructure as a service into two pieces:
\begin{description}
	\item[$\bullet$ Virtualization software security] including all the software pieces that implement the virtualization, including hypervisors, paravirtualization etc. This layer is maintained by the service provider.
	\item[$\bullet$ Customer guest OS or virtual machine] virtual machine running some operating system and software stack. This is maintained by customer.
\end{description}
Above could also be considered another way of describing the the division of responsibility. This is essential part on the other service delivery models as well.
\subsection{Security aspects in Platform as a service}
Key differentiation between infrastructure as a service and platform as a service is that service provider maintains both the hypervisor and the guest operating system patching and configuration.
Assuming that the above is met by the service provider it is safe to say that more current system software is being used and there are scalability gains to be had. Also, lower administrative overhead can be achieved by moving some of the maintenance burden from in-house staffers to service provider. (Jamsa, 2012). Scalability could be seen as a security enhancing feature against certain kinds of attacks, such as denial of service while lower administrative burden might allow staffers to improve software quality as they may have more time available.
In Cloud Computing Kris Jamsa highlights the concern of risk of breach by the platform as a service provider. Its stated that if the service provider fails to be compliant with the service levels, performance availability and security of the application running on platform as a service might be at risk. (Jamsa, 2012).
Michael P. McGrath states in his Understanding PaaS book that its not so much about platform as a service being fundamentally different, but customer just does not see all the actions taking place behind the scenes, such as monitoring, tweaking and constant improvement. He also states that while platform as a service might not work for all use cases, it still works well and can be used to improve the security of a significant portion of the computing stack required for applications. (Michael P. McGrath, 2012).
Aforementioned statements are escalated when combined with the statements by Tim Mather, Subra Kumaraswamy and Shahed Latifas in their book Cloud security and privacy where it is said that service providers do not in general share the configuration details of their security controls for platform as a service systems. This includes operating systems and the processes that are used to secure the hosts implementing the platform as a service -concept. Reasoning being that attackers could possibly utilize this information to implement attacks. (Tim Mather, Subra Kumaraswamy, Shahed Latif, 2009)
\par
All the above points to the direction where customer does not need to implement the host level security but it is good to keep in mind that once again it is still the responsibility of the customer to get the correct level of assurance that the service provider complies with any possible requirements customer may have. (Tim Mather, Subra Kumaraswamy, Shahed Latif, 2009)
\subsection{Security aspects in Software as a service}
Software as a service typically presents itself as a application hosted and developed by service provider and delivered over web browser. This allows customer to limit their needs of on site data center based software and applications leading to less amount of administrative burden. (Jamsa, 2012). Kris Jamsa also states that as software as a service is likely multi tenant this may lead to a situation where any customizing software as a service delivery might difficult, expensive and in some cases impossible. (Jamsa, 2012)
Given that everything from physical hardware, hypervisors and application is hosted by service provider they also have visibility to all of the information of all of the customers of their software as a service offering. (Halpert, 2011)
\par
In addition to everything stated above, the last statement concerning platform as a service still holds truth: it is still the responsibility of the customer to get the correct level of assurance that the service provider complies with any possible requirements customer may have. (Tim Mather, Subra Kumaraswamy, Shahed Latif, 2009)
\subsection{Methods of improving security and availability in cloud}
In order to operate and use cloud in secure and efficient manner both customer and service provider have to plan in advance. When aiming for a complex environment it is essential to look ahead and try to consider the methods and procedures that are required for operation. While it might be possible to implement small cloud service without much planning, anything more substantial requires significant planning and design. Failing this will usual lead to increased costs or worse. (Winkler 2011)
What is good to point out is that every decision, security related or otherwise, will be a tradeoff between options. Tradeoff within security are at times not realized in a sense of those tradeoffs having any impact on security. As an example, bulletproof vest protects against gunshots, so why dont everyone put on a bulletproof vest before heading out? After all, likelihood of being shot is greater than zero? The obvious reason is that this likelihood of being shot is vanishingly small, besides, bulletproof vests are cumbersome, uncomfortable and hot, just to name few downside. Not o mention unfashionable. So, we decide that the unlikely benefits do not justify the downside. This same principle apples when choosing controls for cloud deployment, and if to transfer some of the responsibilities to cloud service provider. (Halpert, 2011)  
\subsubsection{Description of defense-in-depth}
Defence-in-depth is understood as a construct with multitude of related organizational actions and measures that are applied in order to minimize incidents and security compromise. If defence-in-depth is successfully utilized, the reliability, resilience and robustness to withstand attacks is also increased. Concept of defence-in-depth could be split into individual components that are defined as zones that aim at improvinf the selected aspect of the larger whole, for example identity management and availability management. By splitting the big picture into smaller zones it is said to be easier to understand the larger requirements and hence to identify appropriate controls to deploy in the environment of a particular organization.(May, Hammerstain, Mattson, Rush, 2006)
\subsubsection{Service level agreements}
Service level agreements also known as SLAs are sets of condition and terms defined in contracts between customer and the service provider. SLAs can be used to define and agree upon the service levels between provider and customer, including sanctions if the terms are not met. Conditions and terms in SLAs can include various technical, commercial and business service level objectives (SLOs) combined with mechanics of how to measure that the agreed upon services levels are met. (Stamou,2014) Another definition to the service level agreements is given by Xingming Sun,Zhaoqing Pan and Elisa Bertino in their paper "Cloud Computing and Security" in 2018 where they define service level agreement as means to assure quality, reliability, security and scalability of the cloud service. (Xingming Sun,Zhaoqing Pan and Elisa Bertino, 2018) 
\par
To successfully utilize SLA as a way to improve service availability and security the SLA life-cycle could be split into four parts as follows: (Stamou,2014)
\begin{description}
        \item[$\bullet$ Creation of the SLA including contract]
        \item[$\bullet$ Implementing the SLA]
        \item[$\bullet$ Enforcement and monitoring of the SLA]
        \item[$\bullet$ Termination of the SLA]
\end{description}
Generally speaking the first step consists of service provider predefining a set of various SLA levels for customer to choose from and to bind the contract upon. These could be considered as templates for the SLA. Customer then reviews these templates, selects one possibly modifying it and sends it back to the service provider for a review. Service provider then accepts, declines or sends modified version to the customer for a review. (Stamou,2014 s 13). Rest of the SLA life-cycle consists of implementation, regular reviews and eventually ending of the SLA as stated above.
\par
What makes the SLA for cloud specially tricky is the fact that currently SLAs for cloud lack standardization. This is not optimal as standardization would lead into more structured content of SLAs. In a perfect world the SLA should take into account the individual risk requirements of the customer but this can lead into highly tailored SLAs. (Stamou,2014 s14).
Be that as it may, ideally it would be appropriate to consider security similarly to other terms of the the contract, meaning that customer would be able to be aware of what sort of security systems are implemented to safe guard their data and service. Enf result of this would be a something along the lines of security as a service, delivered under an SLA just like any other part of the complete service. Similarly to the lack of standards for cloud service level agreements, there currently are not that many models for service level agreements that would focus on security as majority of service level agreements are focused on performance and availability. The problem is many folded as there is still the need of specialist knowledge to translate the security requirements into appropriate low level security controls that can be enforced and monitored so that the service level agreement is met. This monitoring problem is even more difficult to tackle in cloud environment than traditional IT outsourcing as there are different deployment models - likes of IaaS, Paas and SaaS - where the underlying responsibility is shared between customer and service provider in varying ways. Traditional SIEM, IDS or vulnerability assessment system might not be sufficient in the cloud.(Valentina Casola, Alessandra De Benedictis, Massimiliano Rak, 2015)
\par
Contradicting the whole cloud computing paradigm of on-demand and self service, currently many of the standard contracts available from cloud service providers are rather one-sided with little room for requirements from the customer, meaning that service providers are trying to avoid any meaningful commitments or assuming any responsibility, this lends itself to standard contracts being very service provider -friendly. This is highlighted by permitting unilateral termination or suspension of the service and they also tend to avoid most of the liability of the service provider.(Valentina Casola, Alessandra De Benedictis, Massimiliano Rak, 2015)
\subsubsection{Supply chain security}
\subsubsection{Human factor in security}
\subsubsection{Encrypt static data and in-flight data whenever possible}
As stated earlier the very nature of cloud computing relies in the resource pooling, this on its own poses the question if cloud storage service is suitable as several customers utilize the same storage system. To lessen the risk of data leakage within the cloud customer should implement encryption to protect static data. For infrastructure as a service environment that could mean using encryption methods provided by service provider or encrypting the data using third party system. When using encryption it is essential to use appropriate encryption algorithms, at the time of writing this includes AES, customer should select the encryption algorithm based on actual need and so that it is compliant with regulations. Encryption highlights the importance of managing the encryption keys in a efficient manner and customer should implement a standardized method of user key management and distribution method so that they can utilize the encryption and manage data in a secure fashion. (Xingming Sun,Zhaoqing Pan and Elisa Bertino, 2018)
I in their paper on "A Formal Security Analysis of the Signal Messaging Protocol" from 2017 Katriel Cohn-Gordon, Cas Cremers, Benjamin Dowling, Luke Garratt and Douglas Stebila illustrate the use case for end-to-end encryption of communications.
They state that in the past there has been attempts to improve security by encrypting the messaging between customer and service provider, while this provided some security it still allowed the service provider to access the messaging in plain text.
To overcome this there has been a push for mechanisms that authenticate the customers end nodes using either public keys or pre-shared secret to obtain end-to-end confidentiality and integrity.
While these attempts at end-to-end encryption have been novel, there are apparently some track record of problems related to key management, example being Apple's iMessage where users have no means of manually verifying the keys of their contacts and that there have apparently been flaws in the key management that undermine the security (Katriel Cohn-Gordon, Cas Cremers, Benjamin Dowling, Luke Garratt, Douglas Stebila, 2017).
\par
This just goes to say that key management is of extreme importance and customers should pay special attention to this if they embark on encrypting their communications and data. This is especially true as regulatory requirements like PCI-DSS, ISO 11568 state that key management must be implemented. Proper key management should cover the whole life cycle of the keys and that different application requirements do not contradict the key management process, leading to vulnerabilities.(Mike Andreasen, Troels Norgaard, Alina Mot, Per Snowman, Axel Buecker, Carsten Dahl Frehr, Soren Peen, W. Craig Johnston, 2014)
\par
Vic Winkler gives us a list of most common mistakes when dealing with encryption in the "Securing the Cloud" -book:
\begin{description}
	\item[$\bullet$ Not using encryption] when it would be a viable option.
	\item[$\bullet$ Failing to use encryption] with protocols that have encrypted counterparts. For example FTP, telnet or HTTP 
	\item[$\bullet$ Grand(false) ideas of being a cryptographer] and implementing his/her own algorithm. 
	\item[$\bullet$ Reinventing a wheel ]by trying to implement known algorithm instead of using a proven implementation
	\item[$\bullet$ Including ]password inside a binary, configuration file etc.
	\item[$\bullet$ Storing keys ]together with data being encrypted
	\item[$\bullet$ Failing the bus test]: What happens if the few critical individuals with the keys suffer a disaster while sitting in the same bus? 
	\item[$\bullet$ Distributing] sensitive data via unencrypted email
\end{description} 
Vic Winkler also reminds us that development of cryptographic algorithms is a highly specialized and challenging problem and correctly implementing cryptography in software is almost equally difficult task. Even commercially available products utilize encryption in flawed manner and even single flap in cryptography mey undermine the security of the entire chain of trust. Cryptography is also an area where products have been shown not to work as expected and there is a long history of products that are flawed or that use algorithms that have not been subjected to peer review or test of time. It is especially essential to steer clear of products that rely on secret cryptographic algorithms, instead customer would do wisely by selecting products that implement open and recognized algorithm that has passed the test of time and that have been peer reviewed.(Winkler, 2011)
\subsubsection{Encryption key management}
Management of encryption keys has been a long standing challenge in the communication networks too. Miliatry and intelligence offices have spent better part of the 20th centurity in their attempts at trying to understand the strengths and weaknesses of different approaches on doing key management. The problem is two fold:
\begin{description}
	\item[$\bullet$ Make] sure that right people have the correct crypto keys
	\item[$\bullet$ Ensure] sure that wrong people cannot gain access to any keys
\end{description} 
The real problem is that it is difficult to keep those two in a reasonable balance. In attempt to solve this key management systems were developed that ensure that the keys are changed regularly as this will lessen the likelihood of cryptoanalysis via means of making it more difficult to gain enough ciphertext to break that specific key. In addition, beneficial side effect is that this also limits the damage caused if the key was to be leaked. Another approach is to change the key when the entity holding the key no longer should have it.(Smith, 2015) 
\subsubsection{Information hiding}
In addition to traditional encryption customer might consider another approach known as information hiding. This is a radically different concept towards the same goal as encryption, instead of openly trying to secure given piece of information this approach aims to hide the data inside "junk data" or split the data so that attacker needs to have multiple pieces of data in order to gain any information of value. This concept is apparently actively used in systems such as nuclear weapons where at least two different persons are required to turn two separate keys at the same time to activate something. Underlying principle being that attacker can only control the asset (in our example before, a nuclear weapon), data or message if they can find it. Examples of this could be sound files or images that can be shared openly in the Internet. They could provide possibilities to hide even large messages in the noise of those images or audio files, and it might be unlikely that anyone can expect to find the information. It it stated that roughly one eighth of an image file could be utilized to store information without significantly impacting the quality of the original image file. Add that to the concept of splitting the information into number of parts stored in different locations and one might have a reasonably good way to make information disappear. This approach can also be utilized to create redundancy, for example 3-bit error-correcting codes can be used to recover the secret if one of the three parts is changed. (Peter Wayner, 2009)
\par
One reason why information hiding might be especially interesting in cloud scope is that cloud can provide relatively low-cost and location independent environment to store data. Also, given the underlying idea of cloud being available from anywhere at any time (Kan Yang,Xiaohua Jia, 2013), cloud might provide interesting opportunities for information splitting, especially when data is being split into parts that are stored on different cloud service providers.
\subsubsection{Searchable encryption}
To preserve confidentiality of data, it should be strongly encrypted whenever it is not within the secure boundaries. (Katakri) Thus it it is a natural conclusion that the data should be encrypted during the data transfer to and from cloud as well as while the data is at rest, if the cloud provider can not be considered as fully secure. The problem with this kind of setup is that not much processing can be done without breaking the encryption.
\par
To search for documents stored encrypted in cloud, a searchable encryption system could be applied. In a system taking advantage of this technology, the data is originally stored encrypted in the server. Several searchable encryption systems exist which can make it possible to make for example a keyword search against the document database. Also this search query is encrypted by the user with a specific key. Thus, if the encryption system is applied correctly, any clear text data should not be available for unauthorized users at any point of storing, searching or document retrieval.
Hoang Pham, Jason Woodworth, and Mohsen Amini Salehi. Survey on Secure Search Over Encrypted Data on the Cloud. https://arxiv.org/pdf/1811.09767.pdf
\subsubsection{Data redundancy}
Had the cloud storage been an option at some decades ago, it would have been quite unlikely that we would have the same backup processes and mechanics that are actively in use today. Be that as it may, the fact is that cloud did not exist and enterprise IT departments had to make do with what was available to protect their data against various threats, this includes everything from natural disasters and computer viruses to human errors. This led to the best practices of taking backup copies and storing those into off site locations from which the data could be restored if needed. (Marc Farley, 2013) 
There are at least three methods providing data redundancy: replication, backup and encoding redundancy. Cloud service provider may replicate virtual machines to several locations and so called availability zones for implementing policy or service level agreement to increase availability and disaster recovery capability. (Raghu Yeluri, Enrique Castro-Leon, 2014) Surely enough, cloud computing may provide protection against certain disasters in addition to replication by delivering online data copies to another, alternate location. This may save significant amount of money in a form of not requiring purchase of redundant hardware and software, while still allowing cloud user to recover in case of a harmful event takes place. (Ronald L. Krutz, Russell Dean Vines, 2010)
\par
As same principles apply to cloud backup as to normal backup it is reasonable to review the traditional meaning to data backup. Backups are understood as a snaphost duplicates of the data taken at a certain point in time, stored in some usable format for a given period of time defined by their usefulness in case of need for a restore. There are few different types of backups that can be created, full backup being the representation of the complete data set and full backups are used as a baseline for other kind of backups. Differential backup captures data that has changed since the last full backup while incremental backup captures data that has changed since any kind of backup regardless if it has been a full backup or a differential one. Given the definitions of the different backup types it is easy to assume that incremental backup is the best choice, but it has a downside when it comes to restores: it might require several backup images to restore a given set of data depending on the times when different files of that data set have changed.(Nelson, 2011) One notable exception to this rule is the synthetic full backup that by definition means that multiple partial (incremental or differential) backups are aggregated in the background to create a backup set that represents a view of the data if a full backup was ran instead of partial one.(Farley, 2013). Synthetic full backup might make restoration far simpler than from partial backup, depending on the software.
\par
More cloud-like definition of data redundancy is defined as both copy and encoding redundancy. To further explain these coding redundancy is used during the data access process if data is damaged while copy redundancy can be utilized when data is damaged or lost after once it has been stored. One common approach of encoding redundancy is the erasure coding that relies on the principle of: n file block data is generated as n + m coded data blocks with the erasure code data redundancy is k. (k = m/n), finally store the n + m redundant coded data to multiple cloud storage facilities, final result being that any n blocks can recover the original data. It should be stated that there are multiple erasure encoding methods and many algorithms as erasure codes are an open platform. (Xingming Sun,Zhaoqing Pan and Elisa Bertino, 2018)
\par
It should be noted that while replication and backups give service provider the ability to comply with service level agreements, they also include a risk of dispersed copies of data, credentials and so on floating in the cloud. In addition to ensuring that the aforementioned does not happen customer should make sure that if they decide to change service provider the backup copies and replicas are destroyed according to the agreement. This can be difficult to achieve as there are no standard means of proving that certain dataset is actually properly destroyed.(Raghu Yeluri, Enrique Castro-Leon, 2014)
One problem worth highlighting is the issue of medium and technology obsolescence. This is referring to new backup mediums developing and surfacing and customer having to make sure that the backups stored to old mediums are still readable when required. This is especially problematic with long term data storage or archiving but it is worth noting nonetheless.(Marc Farley, 2013). In cloud medium obsolescence could take place in a form of a storage protocol or proprietary format disappearing from the service offering.
This can happen when utilizing proprietary deduplication mechanisms in order to save money. The processes of deduplication aims on reducing redundancies that can be created in many ways, such as user copying a file and then making small changes to the copy and sharing these files with multiple persons within the client environment. To save capacity, backup (or production, for that matter) software can store only unique data, be it a complete file or a chunk of a file,  and replace redundancies with indexes, pointing to the actual data. (Daehee KimSejun, SongBaek-Young Choi, 2017).
\subsubsection{Authentication}
In the "Cloud security: A comprehensive guide to secure cloud computing" from 2010 Ronald L. Krutz and Russell Dean Vines gave us the following definition: As usual, authentication and identification play major roles in most access control systems. To better understand what this is about lets give both of those terms a set of definitions and what to look for in relations to cloud environment.
Identification could be understood as user giving the system something to establish accountability on, this usually is understood as username or logon ID to the given environment. Username or logon ID should not consist of users real name, job title or function, this is to limit the information available to potential attacker if they ever gain the knowledge of usernames. Authentication on the other hand is the means of making sure that the identity given is the correct one, this is commonly implemented by using password. Authentication constructed of the three types listed below:
\begin{description}
        \item[$\bullet$ Something the user knows], for example password or PIN code
        \item[$\bullet$ Something the user has], for example smartcard or token
        \item[$\bullet$ Something that is unique to each user], for example physical fingerprint or retina scan
\end{description}
It is also possible to combine some of the above authentications mechanics and come up with something call two factor authentication. Example of this would be bank automate that requires both the card and the PIN code. (Ronald L. Krutz, Russell Dean, 2010)
\subsubsection{Service life cycle management}
Not only has cloud computing transformed the way services can be quickly deployed but it has also altered the way customers may want to implement their service life cycle management.
This could be illustrated by the concept where the traditional model where the customer did the implementation and installation in classical data center has been replaced with a model where the role of the customer is transformed more into a like of an integrator, and cloud service provider taking large portion of the responsibilities related to IT infrastructure. Be that as it may, also in the core of the cloud service life cycle is the key principle where all services must produce measurable value to enhance the business goals and desired outcomes.(Buyya, Broberg, Goscinski 2011)
The life cycle of a service hosted in cloud includes several stakeholders, such as service providers and consumers that take role on the delivery of the cloud based applications and the management of the related services. While the life cycle of cloud service is still largely in a state of flux there is general concensus in the literature concerning the individual phases of life cycle and the requirement for service repository in order to support life cycle activities.
This service repository consists of two main components, registry for storing and managing the metadata related to service, this includes things such as service name, version, provider and description etc.). The second component of the service repository is the mechanism that discovers new services. In practice, this repository could take form of relational database. (Tran, Feuerlicht, 2015)
\par
It is said that the life cycle of cloud service consists of five phases: specification of requirements, discovery, negotiation, composition and consumption. In the requirement specification phase, both functional and non-functional requirements for the service are described, other way to say this is to define the requirement that the given service needs to fulfil. While there are differences in the spec depending on the type of the service, normally the specification shall include technical details, such as service interface (e.g WSDL) but it may also include technological details such as hardware spec or programming languages used. The non-functional requirements include things like availability, performance and security. Service identification is based on both the functional and non-functional requirements specified in the requirements specification phase. Service identification phase utilizes service category hierarchy and the attributes identified in the service requirements stage. These attributes are then stored in to a web based service repository that allowes consumers to search services based on their various attributes. This leads to consumers trying to search for services that are already registered and available in the repository, meaning that they are certified for use. After the appropriate service is found and selected, testing and approval phase follow. Service approval is defined as an internal certification procedure, deciding if the cloud service is certified for use, this can be very time consuming as the selection of services is wide. (Tran, Feuerlicht, 2015) These internal certifications can take for of a rigorous frameworks that are used to evaluate the service capability and risks involved prior to new service is being deployed or old service is being modified. This can include things such as approved service release package, updated service package or bundle, updates in the service portfolio, updates in contracts and new documentation.(Buyya, Broberg, Goscinski 2011) 
\par
In the service integration phase the cloud service is being integrated into the customers enterprise environment and processes. At this stage the service is taken into production, meaning that business processes are executed on the cloud environment. Effectiveness of the operation relies on the ability to detect any deviations or defects fromt he normal operation. (Buyya, Broberg, Goscinski 2011) To this end, monitoring state of the life cycle is defined. As the name states, monitoring takes place during the runtime of the application or service. It is common that both the customer and the service provider implement their own monitoring independently, this is natural as both parties share the responsibility of the availabability of the service. Service repository  is used store runtime performance and availability of the service, this includes things like response time and error messages etc. Maintaining accurate statistics is enables customer to to compare the seen performance to what was stated in the service level agreenment. In addition to monitoring, also service optimization shall take place when the service is in the running phase. This includes software upgrades and possibly changing of the service provider, for example PayPal could be replaced by SecurePay. Optimization phase can also include aspects of optimization of processes internal to the customer organization. (Tran, Feuerlicht, 2015) Both monitoring and optimization contribute to the continous service improvement where the object is to ensure that the cloud service is still feasible option to meet the business requirements. (Buyya, Broberg, Goscinski 2011)
\par
For comparisons sake to the above statements specific to cloud computing we can look into the life cycle management process of a nuclear power plant. Similarly to cloud service life cycle, nuclear power plant starts its life cycle with a design phase, followed by construction and commissioning that eventually should lead into start of operation. Just like in cloud service life cycle, nuclear plant has safety management that aims to improve the safety of the organization by enforcing planning, control and supervision to activities concerning safety, safety management also supports the safety culture by education. As with cloud service, nuclear plants have a preventive maintenance that is performed to detect and mitigate degradation. Properly executed preventative maintenance is seen as essential part of life cycle management. In addition to preventative maintenance, nuclear plants have predictive maintenance that that is performed continously or at given set of intervals, similarly to the periodic safety reviews. Safety reviews assess the symptoms of ageing, compare the original design safety stance to current situation, identify achievable improvements etc. (International Atomic Energy Agency, 2002) 
\par
This sounds quite similar to the cloud service life cycle described above? Nuclear plant life cycle management has one phase that was not mentioned in none of the above cloud service life cycles: Decommissioning phase. According to the nuclear plant life cycle, the planning for decommissioning is integral concept of on the running phase already since it allows time to prepare for the actual decommissioning process and the final decommissioning of the facility in a controlled manner with positive outcomes. It is stated that detailed decommissioning planning should start already 5 years before the planned transition to the actual decommission. Planning for decommissioning should be seens as a part of design and building phases. (International Atomic Energy Agency, 2002). We find this planning for decommission something that should be addressed in cloud service life cycle as well. Infact, BMC software tells us on their "Cloud lifecycle management: managing cloud services from request to retirement" that cloud service might be seen as out-of-sight and out-of-mind, so unless cloud service is not actively placed in the termination queue it will easily linger idefinitely. This is especially true as the goal of the cloud is to improve the usage of resource, this makes service decommissioning important function that actually completes the life cycle. (BMC Software, 2010)
\par 
Key aspect in cloud service life cycle management is the properly designed service repository.(Tran, Feuerlicht, 2015)
\subsubsection{Handling the reliance to connectivity}
Once an application is running in the remote location it is obvious that connectivity is of paramount importance, in essence, having no connectivity in the campus means having no application and this can mean having no business. While many organizations have Internet connectivity these days it is still surprisingly uncommon for organizations to have backup connectivity if the unthinkable disruption happens. Fiber cuts are not all that uncommon (Teddy Hayford-Acquah and Ben Asante, 2017).
To reduce the impact of a last mile failure it is a common practice to have two physically separate lines from a service provider, terminated to two separate customer premises routers in two separate equipment rooms. (Gunnar Bøe, Vidar Faltinsen, Einar Lillebrygfjeld, 2011) Two routers using VRRP protocol act in active-passive manner to provide so-called first-hop redundancy (rfc5798, 2010). This approach, when combined with physically separate lines, provides protection from fiber cuts on the last mile and this also protects from power supply failures in the customer premises router and also acts as a backup connection during router software upgrades and some configuration changes.
However, this method does not protect against catastrophic failures in the service provider network. To accomplish this, it is required to have similarly separated lines and routers from two separate service providers. Assuming that the customer has some IP block(s) to announce over BGP and that the service providers are accepting the customer IP block(s) for transit it is possible to create fully redundant last mile connectivity. All these relatively complex and expensive requirements are likely the reason why organizations won't purchase redundant connectivity but instead accept the risk of significant business impact and downtime. (Packetworks, 2016).
\par
Similarly, if cloud application is running in a "stretched" network infrastructure, e.g. data center interconnect, it is essential that the interconnect is built in a redundant fashion. While redundancy is all good it can also cause failures of different kind, but with equally potential for catastrophe (Pepelnjak, 2011).
This problem with location redundancy could be solved by making the application layer not so reliant on the underlying IP layer, this could be done for example by decoupling the service IP - address that end users connect to - and advertising it to data center routers via BGP over only locally significant subnet. Even while there are tools for this sort of decoupling (RIPE 2010), this kind of approaches have apparently been deemed as non-trivial and time consuming tasks so currently it would appear that the accepted solution is to introduce more complexity outside the application to hide the underlying already existing complexity of IP transport. One such method is overlay networking, such as VXLAN, that builds up a stretched OSI layer 2 domain over routed network (rfc7348).
While there are quite a few methods of implementing encryption in network it should be questioned if it is a sustainable choice to outsource application security to network layer. Implementing encryption using IPSEC (rfc4301) commonly indicates that OSI layer 3 routing should be implemented between data centers, while doing routing is a healthy choice for data center interconnect in terms of limiting failure domains. It also means that an overlay networking is likely required if OSI layer 2 transparency is insisted upon. To implement both OSI layer 2 transparency and encryption one could choose to do encryption on OSI layer 2 via MACSEC (Juniper, 2018), VXLAN over IPSEC or by utilizing encryption in DWDM (Arista, 2018) level. It should be noted that both MACSEC and IPSEC have impact in the capacity of performance in terms of payload transferred versus the capacity utilized. Running VXLAN over IPSEC may have an impact in the net payload as well, or at least MTU should be carefully considered. Many public cloud providers such as Amazon (Amazon 2018), Google (Google 2018) and Microsoft (Microsoft 2018) support IPSEC tunnels to tenant specific virtual routing and forwarding instances that are logically separated from one another. Still, it is worth mentioning that even if the data center interconnect from customer data center to cloud provider is encrypted this does not mean that the internal data center traffic inside the service provider facility is encrypted in any fashion. This is one reason why it might be a good idea not to rely on network level to implement the encryption, instead utilize sufficient encryption in the application level, just to be sure.
\subsubsection{Network as part of the defensive arsenal}
Network is said to be the first layer of defense in the defense in depth mind set. This is also true for cloud services as the network is still the first contact point for the attacker towards the cloud environment. (Zeal Vora, 2017)
Network can also provide ideal visibility to the traffic, depending on where the security appliances are placed. In a perfect world there would be security device in each and every ingress and egress point into and onto the network, this would include things like Internet upstream, possible peerings, MPLS links, encrypted vpn links and so on. Placement of the security device should also take into consideration the possible private IP addresses being used, as it is useful to be able to easily identify which internal ip address is part of the possible alert or packet filtering rule. (Chris Sanders, Jason Smith, 2013)
\par
This makes it important to take security into an account in addition to the robustness described in the previous chapter. Generally security on network layer is built using firewalls, intrusion detection (and preventation) systems, virtual private network (VPN) gateways utilizing encryption, and segmentation to demilitarized zones just to name few. Let us next investigate the basic principles of few of the aforementioned technologies and where they might fit in the larger security landscape, starting with firewall. Firewall is a device that sits between the client and server, whenever client is requesting something from the server the request is first seen by the firewall. Firewalls rely on rules to define what kind of requests are allowed, these rules are based on ports, protocol names (and numbers), ip addresses and flags. There are approximately two different kinds of firewalls: stateful and stateless, difference being that the stateful firewall keeps track of the connection status, meaning that it knows about the three-way-handshake of TCP, while stateless firewall relies on matching rules against each individual packet. Firewall, in its classical meaning, does not understand the upper layer protocols, this is where IDS or IPS comes in. These techniques usually rely on signatures to identify packets belonging to known attacks and they can then make decisions if particular packet is to be allowed or denied or if an alarm needs to be raised. IDS can be a complementing feature on a firewall as well. In cloud context, IDS can be implemented by using cloud service providers traffic mirroring features. Meaning, that cloud instance would replicate all the traffic it receives and send it to a central IDS for analysis. This has a known drawback of leading to a large volumes of traffic at peak times.(Zeal Vora, 2017)
\par
Even higher in the stack after IDS/IPS and the TCP headers is the web application firewall (WAF). WAF can be used when something is required between the client application and the servers and network layer and transport layer are required to be left open. This is the case with typical web application that is supposed to be accessible from everywhere in the Internet. WAF is supposed to understand HTTP protocol, SQL, XML and cookies, all part of the application server or even the application itself. To implement WAF efficiently in depth knowledge of the application being protected is required.(Zeal Vora, 2017) Virtual private network is completely separate concept of the two mentioned above. VPN is transport mechanism that creates encrypted tunnel across Internet, idea being that plain text packets can be transported over Internet in a secure fashion. In cloud context virtual private network could be used for example to migrate virtual machines between security domains.(Zheming Xu,Sheng Di,Weida Zhang,Luwei Cheng,Cho-Li Wang, 2011)
\par
NTP??
\subsubsection{Virtual machine image management}
To discuss the security aspects of virtual machine disk images, lets first establish what is meant by virtual machine disk image.
Base images of virtual machines ran inside a cloud environment come in two basic form factors: disk images and container images. Disk image represents the underlying hard disk of the virtual machine and there are quite a few different formats to this.
\begin{description}
	\item[$\bullet$ raw] is an unstructured image format
	\item[$\bullet$ qcow2] is the format-of-choice of the QEMU system, this supports dynamic expansion and copy-on-write
	\item[$\bullet$ vhd] is a format accepted by various proprietary systems and KVM amongst others
	\item[$\bullet$ ARI] is the amazon kernel image
\end{description}
There are others as well, such as vmdk and vdi. Container format on the other hand contains also required metadata about the virtual machine itself in addition to the disk image, it is not just the backing hard drive.(Raghu Yeluri, Enrique Castro-Leon, 2014)
\par
These images may be provided to the customer by the cloud service provider as pre configured virtual machine images, or the customer may provide their own, or even download the images from somewhere in the Internet. Customer should not assume that the pre configured container images or virtual machines are secure or compliant with the customer requirements, regardless if they are provided by the cloud service provider or especially if downloaded from the Internet. In addition to this, it is left to the customer to deal with the patch management of these images as the service provider only takes care of the underlying infrastructure. This has to be seen as a continuous effort, not just as something that takes place only during the initial roll out.(Vacca, 2016)
\subsubsection{Vulnerability and patch management}
As stated above the responsibility of the security posture of virtual machines provisioned in Infrastructure as a service cloud deployment is on the customer. (Vacca, 2016) Be that as it may, customer should still make sure that the service provider has a policy to upgrade and patch their own systems in timely and safe manner in order to limit exposure. (Winkler, 2011)
It is regarded as a best practice to regularly run vulnerability scanning. Generally speaking, there are two different types of scans, ones that are initiated outside of a machine, over the network. These scans can target any device reachable, and they do not assume access to the target system. Second, more through scan requires access to the target system in order to create complete inventory of what the target system has in store, these authenticated scans may take significant amount of time. Ideally, these scans should include everything from cloud management platforms, servers and possible network devices. Idea of the scan is to identify new or left-over vulnerabilities so that the relevant risk might be mitigated. These scans can also be utilized to cross check the found devices against the catalog of known devices, if unknown devices are found then more thural investigation is required. Scans can also help to identify missing patches if the service is reachable over the network. (Winkler, 2011)
\subsubsection{Log management}
Lets start by giving a definition to what we mean by log management. Services, operating systems and applications usually have some means to provide information on errors, warnings and events related to security, for example users loggin in and out of a system. These events are stored in to concept of log entries, that in turn are the main content of a log file. Reasoning behind logging is to use those log files to analyze, debug and optimize systems and service, but in addition to that they can be used to detect security compromise or attempts at it.
Problem with the logging systems is that quite often they are not configured ideally, meaning that essential messages may go unnoticed in the large stream of messages that are caused by events of low importance or even those that are completely irrelevant, other problem being that users of these logging systems might not even know where to begin their search for specific logs, or how to configure the logging mechanics to begin with. Luckily enough, there are tools that support users in their task of trying to keep track of log files, some tools can even analyze the log files on their own to some extent. These tools are essential since it is of particular importance to filter the logs for both reasons: to summarize the events and also to identify suspicius or even dangerous actitivy. Usability of these tools can be improved further by configuring automater alarms or in some cases, automated counter measures when these is sufficient evidence that there is likely malicious actions happening. (Basin, Challer, Schläpfer, 2011)
\par
In addition to technical reasons, organizations often try to configure their logging to meet certain audit criteria, for example PCI DSS, being an industry standard followed by anyone that handles credit and debit cards, requires that organization keeps track of all access to resources and cardholder data. Another example could be HIPAA (Health Insurance Portability and Accountability Act from 1996) that has rules regarding the system logging. And it does not end there, as many of the criteria expect not only logging but also monitoring of the logs.(Smith, 2015)
\par
Given the nature of the log files and their purpose it is essential to safe guard these logs themselves against compromise as attackers may try to modify log entries in order to cover their tracks.
There are several approaches to increase the security of the logs. One approach to improve log file security is to set the log files in append only mode, meaning that log files can be on modified from the end of the file, making it impossible to modify or delete the log entries. Also, it is advised that log files are readable only by the system administrator as this makes it more difficult for the attacker to construct on idea of normal traffic, hence making it more difficult to hide their activities in a flood of regular activity. The two previous security improvements are based on the idea that attacker has not compromised the system storing the logs.  One being using a separate logging server(s). This can be efficient as it decouples the logs from the server or application that produces the service, as an example email, DNS and httpd logs could be copied to a central log server. End result being that the attacker would have to compromise both the service and log server, this allows log server to implement additional security controls. To harden the remote logging approach it is feasible to store the log server hostname and ip-address in the /etc/hosts file, just in case the attacker manages to distrubt DNS, leading to difficulties in sending the logs to the remote host. It is also recommended to utilize encryption when sending the logs whenever possible, this can be accomplished using covert channels, such as tunneling the log messages. To further improve the security of log files, one could consider storing the logs to write-once medium, such as CD-ROM. Performance of these mediums if noticeable different especially if considering doing this realtime, better approach might be to flush the logs to CD-ROM regularly, for example once a day, or when given criteria such as size of a log file is met.(Lantz,Hall,Couraud, 2006)
\par
In addition to securing the log servers much effort has taken place to come up woth such cryptographic mechanisms that could same time resist attackers that have gained full control of the logging system that holds the secret key, and on the other hand, still continue to function in order to help exposing the illicit log modifications that might have taken place prior the attacker managed to get his hands on the secret key. Main idea of all this being that it would not be possible to modify the logs without being noticed. Since this is seen as impossible using regular there is a proposal for forward-secure schemes. These take the assumption that time shall be devided into intervals, known as epochs, they use different secret keys for each epoch. For sake of efficiency, secret key for and epoch ist is calculated using the secret key of the previous epoch t - 1, in addition to that there is one verification key. To make the scheme secure, secret keys are to be securely erased when they expire, this ensures that the attacker cannot reconstruct the signatures of the previous epochs.(Hartung,Kaidel,Koch,Koch,Hartmann, 2017)
\par
Given all the above, Customer would do wisely to discuss the approach the cloud service provider has taken concerning the the above log management aspects and if they are in compliance with the regulation and criteria expected. On the other hand, customer implements his own logging and log management for the application being delivered inside cloud, these also need to be aligned with the requirements.
\section{Self-assessment of cloud security posture}
Before going further, lets briefly address the concept of criteria-based self-assessment and how one might approach the whole concept of self-assessment. 
As an example, in educational environment, students can utilize the self-assessment as a means to take greater responsibility of their own studies as they get to evaluate their own work. This gives a student a opportunity to themselves to detect the areas where they need to improve upon.
Via this, they can gain the opportunity to improve their studies independently, in a responsible way and also to monitor the evolution of those studies. (Tiia Kokkonen, 2012).
\par
Just by reading the above statement and by replacing the words "student" with "cloud user", "education" with "IT" and "study" with "application" we can see that regular self-assessment works fine with the idea of continuous improvement and when preparing for audits.
Self-assessment should not be seen as a exercise in weakness finding that just consumes resources, Tiia Kokkonen points out that students should concentrate on the benefits of self-assessment.
Similarly, an internal audit or self-assessment ran by the organization itself could be seen as means to improve the constantly developing security policy, using the finest controls for the currently known risks that the organization may afford.
The attitude of self-assessment, be it for studies or IT, could also be illustrated by as follows: Negative culture pushes persons and organizations towards avoiding getting blamed for mistakes and managing just up to the letter of the law, not one bit further.
Culture of safety on the other hand, is all about preventing the bad thing from taking place, this includes things like transparency and continuous improvement. Also, in a safety culture, everyone acts as a some sort of internal auditor, e.g spotting flaws and areas of improvement in order to promote safety. Mistakes and findings are not to be used as means to blame someone, but instead they are to be learned from. (Raymond Pompon, 2016)
\subsection{Difference between self-assessment and audit}
As stated, self-assessment means observation and evaluation of oneself or activities, viewpoints and performance of one's capability, performance or ability at a given task in relation to on objective standard. The important bit here is that self-assessment is done by oneself, not by an external party (Oxford dictionaries, 2018). This is a key differentiation to audit and compliance, as described next.
\par
Prior to going deeper into self-assessment of cloud security posture we need to define what is meant by compliance and audit, and how they differ from self-assessment.
The classical definition of compliance is to meet a requirement, yet in the context of security compliance is a security blueprint for certain type of data. Organization that owns the data defines the minimum level of security.
Audit on the other hand is the process that measures how the organization is aligned with the given compliance requirement at a given point in time. (Prashant Priyam, 2018)
Another definition of the term auditing refers to the accounting of user activity on data. This can mean read and write operations, who did them and when.
Cloud offers multitude of options to provide security, yet it largely depends on the requirements and talent available to implement those security features in practice. It is key to understand that the implementation of security in cloud is slightly different from what it is with on-premise or traditional deployments. (Prashant Priyam, 2018)
Oxford dictionary definition of audit states that the audit is an official inspection of entitys accounts by an independent auditor. (Oxford dictionaries, 2018). Another definition of audit is given by Ben Halpert in his book Auditing Cloud Computing: A Security and Privacy Guide as follows: Audit is an method of assuring that certain standard or practice is implemented and this is done by the auditor systematically examining the evidence for the compliance against given criteria. (Halpert 2011). We believe that the aforementioned statements highlight the difference of audit and self-assessment. 
\subsection{Risk analysis: Selecting targets for assessment}
In general, decisions related to compliance and security would ideally be backed by risks. To this end, it is important to have accurate understanding of the risks one is ascertaining, as this will make the organizations security policy more effective.
For this to happen it is essential to understand the concept of risk. Risk could be defined as a possibility of suffering harm for particular asset, asset being anything of value. Value of asset could be interpret as time and resources required to rebuild or restore the asset to its former state. Vulnerability on the other hand is a known weakness in that particular asset that could lead to the exploitation of the asset in question. All of this can be put into a form of a equation: risk = threat x vulnerability + asset value.(Bejtlich, 2004)
In practice this means that one should identify the key assets, how to handle them and what risks they may pose before spending any money on security. This is is called risk analysis and it can be used to identify where the organization should put their focus on in terms of security. Risk analysis will likely not be perfect given the apparent fuzziness of measuring risk, yet it will provably be better than any guesswork, especially when adapted to each organization and repeated at regular intervals so that it matches the reasonably current situation. Valuable risk analysis could be defined as realistic, actionable and reproducible.(Raymond Pompon, 2016)
Unfortunately, cloud also present extra challenge when doing risk assessments compared to traditional IT. This represents itself in a form of cloud service providers generally keeping the locations, architectures and other security details of their environment confidential from cloud consumers. This makes it difficult for customer to assess the threats, risks and vulnerabilities of those environments. In addition to this, service provider also have to prioritize the problems they solve first as risks are realized, these priorisations might not be openly communicated. These lead to a situation where customer has to rely and trust on the service provider when doing their risk assessments. Net result of this should be that the accountability and trust are mandatory factors to consider before customer should go forward with a cloud approach. (Cayirci, 2015)
Other way to look at the extra challenge of risk management with cloud is to say that cloud service providers design and implement their architectures and services to fit the requirements of a large pool of potential cloud customers in a way that requires the smallest possible amount of per customer customization. Ideally, the cloud service providers security and privacy controls are based on applicable laws, directives and standards while being considered for their effectiveness. Service provider does not know the specific requirements and expectations and therefore these controls provided by service provider should be seen as a generic core set. (Vacca, 2016)
\par
Problems aside, according to Raymond Pompon, there are essentially two kinds of risk analysis: qualitative and quantitative. Qualitative method is based on specialists doing ratings on the factors against a scale. This scale could consist of "levels", such as low, medium and high, or colors etc.
Raymond Pompon also points out that due to the somewhat subjective nature of qualitative method it may need some clarifying for the ratings used. One could utilize a table with meanings, such as the following for likelihood:
\begin{description}
	\item[$\bullet$ Frequent] Assumed to take place more than 10 times per year
	\item[$\bullet$ Occasional]Assumed to take place between 1 and 10 times per year
	\item[$\bullet$ Remote] Assumed to take place between 1 time per year and once every 5 years
	\item[$\bullet$ Very Unlikely] Assumed to take place less than once every 5 years
\end{description}
Raymond Pompon also proposes three impact ratings: minor, major and critical. All three of those can also be split into three sub categories: confidentiality impact, integrity impact and availability impact.
Example of confidentiality of minor scale could be under 10 database records of confidential nature being exposed internally without any proof of exploitation, while major impact of the same thing would entail that several internal employees with no authorization having accessed these records. Critical rating would be under 10 data records being exposed externally or more than 10 records exposed internally. Integrity impact follows the same guidelines but it essentially replaces the exposure of data with data being altered without authorization and whether the alteration can be detected and corrected. Availability impact is slightly different concept, minor being several users having no access for 1-5 days, or customer facing service down up to an hour. Major availability impact could be characterized as customer facing service being down for more than an hour but less than a day, critical represents a situation where customer facing service is down for more than a business day. (Raymond Pompon,2016) 
\par
Quantitative risk analysis utilizes real statistics and data instead of subjective specialist opinions. These statistics can be collected from asset analysis and monitoring systems.
Similarly, as with qualitative analysis organization shall match its assets against attack surface, known weaknesses and implemented controls. For example, if company hosts 10 websites (assets) it might know that on average it is missing 2 security patches on each (weakness) and the control against this weakness could be a firewall (control). Another example that follows the same lines would be: 350 (attack surface) users (asset) are subject to social engineering (weakness) but only 4\% failed the last phishing test training (control).
While the above gives us numbers it is likely that in the end something like security steering committee will make subjective calls, but at least they will have best possible data to base their decisions on.
\par
With risk and asset information at hand it is easier to select the aspects to self-assess. 
\subsection{Controls to assess}
According to Chris Jacksons book Network Security Auditing (2010) There are three main categories of controls:
\begin{description}
	\item[$\bullet$ Administrative controls] are made up of policies, training and processes.
	\item[$\bullet$ Technical controls] include technologies such as firewalls, IDS etc. that are used to implement access control.
	\item[$\bullet$ Physical controls] are used to control the physical access to resources, for example locks and fences fall into this category.
\end{description}
These same categories can also be found from the Katakri tool(Katakri, 2015), Similarly these primary control groups can be further split into more granular controls:
\begin{description}
	\item[$\bullet$ Preventative controls] such as firewalls, login banners and policies are used to enforce confidentiality.
	\item[$\bullet$ Detective controls] are in essence alarming mechanisms to indicate that bad things are happening.
	\item[$\bullet$ Corrective controls] can be used to double check that security controls are in place and take actions if needed.
	\item[$\bullet$ Recovery controls] come into play if the bad thing happens. Examples are backup, redundant power supplies and spare parts.
\end{description}
Interleaved nature of the various controls described above provides a way to investigate whether service provider, customer of application being assessed has met and implemented its controls to sufficient level. (Jackson, 2010)

\section{Conclusions}
\begin{thebibliography}{9}
\makeatletter
\def\@biblabel#1{}
\let\old@bibitem\bibitem
\def\bibitem#1{\old@bibitem{#1}\leavevmode\kern-\bibindent}
\makeatother
\bibitem{Systematic Service Level Agreement SLA data management}
	Aikaterini, S.
	2014.
	\textit{Systematic Service Level Agreement SLA data management}
	University of Geneva
\bibitem{Reasons to Think About Cloud Computing}
        Ahlgren, J.
	2012.
	\textit{Reasons to Think About Cloud Computing}
        Metropolia
\bibitem{CyberWar, CyberTerror, CyberCrime and CyberActivism}
	Mehan, J.
	2014.
	\textit{CyberWar, CyberTerror, CyberCrime and CyberActivism}
	IT Governance Publishing
\bibitem{Defense in Depth: Foundations for Secure and Resilient IT Enterprises}
	May, C J.,
	Hammerstein, J.,
	Mattson, J,.
	Rush, K.
	2006.
	\textit{Defense in Depth: Foundations for Secure and Resilient IT Enterprises}
	Carnegie Mellon University
\bibitem{Cloud computing, Tieto cloud server model}
        Suikkanen,S.
        2013.
	\textit{Cloud computing, Tieto cloud server model}
        Saimaa University of Applied Sciences
\bibitem{Safe and effective nuclear power plant life cycle management towards decommissioning}
	International Atomic Energy Agency.
	2002.
	\textit{Safe and effective nuclear power plant life cycle management towards decommissioning}
	International Atomic Energy Agency.
\bibitem{Cloud Computing: Principles and Paradigms}
        Buyya, R.,
        Broberg, J.,
        Goscinski, A.,
        2011.
        \textit{Cloud Computing: Principles and Paradigms.}
	John Wiley \& Sons.
\bibitem{Service Repository for Cloud Service Consumer Life Cycle Management }
        Thai Tran, H.,
        Feuerlicht, G.
        2015.
        \textit{Service Repository for Cloud Service Consumer Life Cycle Management }.
	Faculty of Engineering and Information Technology,.
        University of Technology, Sydney.
\bibitem{LOCKING DOWN LOG FILES:  ENHANCING NETWORK SECURITY BY PROTECTING LOG FILES }
        Lantz, B,.
        Hall, R,.
        Couraud, J,.
        2006.
        \textit{Locking down log files:  enhancing network security by protecting log files}.
	Utah State University.
\bibitem{Elementary Information Security, 2nd Edition}
        Smith.,
        2015.,
	\textit{Elementary Information Security, 2nd Edition}.
        Jones \& Bartlett Learning.
\bibitem{Practical and Robust Secure Logging from Fault-Tolerant Sequential Aggregate Signatures}
        Hartung, G,.
        Kaidel, B,.
        Koch, A,.
        Koch, J,.
        Hartmann, D,.
        2017.
	\textit{Practical and Robust Secure Logging from Fault-Tolerant Sequential Aggregate Signatures}.
        Springer.
\bibitem{Applied Information Security: A Hands-on Approach}
        Basin, B,.
        Schaller, P,.
        Schläpfer, M,.
        2011.
	\textit{Applied Information Security: A Hands-on Approach}.
        Springer-Verlag Berlin Heidelberg.
\bibitem{The Tao of Network Security Monitoring Beyond Intrusion Detection}
        Richard Bejtlich, R.
        2004.
	\textit{The Tao of Network Security Monitoring Beyond Intrusion Detection}.
        Addison-Wesley Professional.
\bibitem{Applied Network Security Monitoring}
        Sanders, C,.
	Smith, J.
        2013.
        \textit{Applied Network Security Monitoring}
	Syngress.
\bibitem{WAVNet: Wide-Area Network Virtualization Technique for Virtual Private Cloud}.
        Xu, Z,.
        Di, S,.
        Zhang, W,.
        Cheng, L,.
        Wang, C.
	2011.
        \textit{WAVNet: Wide-Area Network Virtualization Technique for Virtual Private Cloud}.
        IEEE.
\bibitem{Guide to Computer Network Security}
        Migga Kizza, J,.
        2012.
        \textit{Guide to Computer Network Security}
	Springer.
\bibitem{Securing the Cloud}
        Winkler, V.
	\textit{Securing the Cloud}.
        2011.
        Syngress.
\bibitem{Cloud Computing Security}
        John R. Vacca
        2016.
        \textit{Cloud Computing Security}.
	CRC Press.
\bibitem{Accountability and security in the cloud : First Summer School, Cloud Accountability Project, A4Cloud, Malaga, Spain, June 2-6, 2014, Revised Selected Papers and Lectures}
        Cayirci, E.
	2014.
        \textit{Models for Cloud Risk Assessment: A Tutorial}.
        First Summer School, Cloud Accountability Project, A4Cloud, Malaga, Spain.
\bibitem{Accountability and security in the cloud : First Summer School, Cloud Accountability Project, A4Cloud, Malaga, Spain, June 2-6, 2014, Revised Selected Papers and Lectures}
        Valentina Casola,.
        Alessandra De Benedictis,.
        Massimiliano Rak,.
        2014.
	\textit{Accountability and security in the cloud}
       	First Summer School, Cloud Accountability Project, A4Cloud, Malaga, Spain. 
\bibitem{The Cloud Security Ecosystem}
        Ko, R,. 
	Choo, R.
        2015.
        \textit{The Cloud Security Ecosystem}.
	Syngress.
\bibitem{Enterprise Cloud Security and Governance}
        Vora, Z.
        2017.
	\textit{Enterprise Cloud Security and Governance}.
	Packt Publishing.
\bibitem{Data Deduplication for Data Optimization for Storage and Network Systems}
        KimSejun,D.,
        Choi, SB.
        2017.
	\textit{Data Deduplication for Data Optimization for Storage and Network Systems}
        Springer.
\bibitem{Rethinking Enterprise Storage: A Hybrid Cloud Model}
        Farley, M.
        2013.
	\textit{Rethinking Enterprise Storage: A Hybrid Cloud Model}
        Microsoft Press.
\bibitem{Security for Cloud Storage systems}
        Yang, K.,
	Jia, X.
        2013.
	\textit{Security for Cloud Storage systems}
        Springer.
\bibitem{Disappearing Cryptography, 3rd Edition}
        Wayner, P,.
        2009.
	\textit{Disappearing Cryptography, 3rd Edition}.
        Morgan Kaufmann.
\bibitem{IT Security Risk Control Management: An Audit Preparation Plan}
        Pompon, R.
        2016.
        \textit{IT Security Risk Control Management: An Audit Preparation Plan}.
	Apress.
\bibitem{The experiences with and opinions on self-assessment among students and their teachers}
        Kokkonen, T.
        2012.
        \textit{The experiences with and opinions on self-assessment among students and their teachers}.
	University of Jyväskylä.
\bibitem{Promoting Learning and Achievement Through Self-Assessment}
        http://www.tandfonline.com/loi/htip20
        Andrade, H.,
        Valtcheva, A.
	2009.
	\textit{Promoting Learning and Achievement Through Self-Assessment}.
	Routledge, Taylor \& Francis Group
\bibitem{Key management deployment guide : using the IBM Enterprise key management foundation}
        Poughkeepsie NY IBM Corp. International Technical Support Organization
        Andreasen, M.,
        Norgaard, T.,
        Mot, A,.
        Snowman, P.,
        Buecker, A.,
        Frehr, C,.
        Peen, S.
        Johnston,W.C.
	\textit{Key management deployment guide : using the IBM Enterprise key management foundation}.
        2014.
	IBM Redbooks
\bibitem{A Formal Security Analysis of the Signal Messaging Protocol}
        Cohn-Gordon, K.,
        Cremers, C.,
        Dowling, B,.
        Garratt, L.,
        Stebila, D.
        2017.
        \textit{A Formal Security Analysis of the Signal Messaging Protocol}
	University of Oxford, UK
        McMaster University, Canada
        Royal Holloway, University of London, UK
\bibitem{Building the infrastructure for cloud security: a solutions view}
        Yeluri,R.,
        Castro-Leon,E,.
        2014.
	\textit{Building the infrastructure for cloud security: a solutions view}
        Apress
\bibitem{Cloud Computing and Security}
        Sun, X,.
        Pan, Z,.
        Bertino, E.
        2018.
	\textit{Cloud computing and security : First International Conference, ICCCS 2015, Nanjing, China, August 13-15, 2015, Revised selected papers}.
	Springer
\bibitem{Pro data backup and recovery}
        Nelson, S.
        2011.
        \textit{Pro data backup and recovery}
	Apress
\bibitem{oxford dictionaries, 2018 }
	\textit{Oxford dictionary or English}
	2018.
	Oxford University Press
\bibitem{Cloud security: A comprehensive guide to secure cloud computing}
        Krutz, RL.,
        Dean Vines, RD.
        2010.
        \textit{Cloud security: A comprehensive guide to secure cloud computing}
	John Wiley \& Sons
\bibitem{Cloud security}
        Bond, J.
        2018.
        \textit{Cloud security}
	O'reilly media, inc
\bibitem{Cloud Computing, Security, Privacy in New Computing Environments,2016}
        Wan, J.,
        Lin, K.,
        Zeng, D.,
        Li, J.,
        Xiang, Y.,
        Liao, X.,
        Huang, J,.
        Liu, Z.
        2016.
        \textit{Cloud Computing, Security, Privacy in New Computing Environments,2016}.
	Conference, SPNCE 2016, Guangzhou, China.
\bibitem{Auditing Cloud Computing: A Security and Privacy Guide}
        Halpert, B.
        2011.
	\textit{Auditing Cloud Computing: A Security and Privacy Guide}
	John Wiley \& Sons.
\bibitem{Understanding PaaS}
        McGrath, M P.
        2012.
	\textit{Understanding PaaS}
        O'Reilly Media, Inc.
\bibitem{Cloud Computing}
        Jamsa, K
        2012.
        \textit{Cloud Computing}.
	Jones \& Bartlett Learning
\bibitem{Cloud Security and Privacy}
        Mather, T.,
        Kumaraswamy, S.,
        Latif,S.
        2009.
	\textit{Cloud Security and Privacy}
        O'Reilly Media, Inc.
\bibitem{Network Security Auditing}
        Jacksoin, C.
        2010.
       	\textit{Network Security Auditing}
	Cisco Press.
\bibitem{IT Security Risk Management in the Context of Cloud Computing}
        André Loske, A.
        2015.
	\textit{IT Security Risk Management in the Context of Cloud Computing}
        Springer Fachmedien Wiesbaden
\bibitem{Prepare for the Worst, Plan for the Best: Disaster Preparedness and Recovery for Small Businesses}
        Childs, D.R.
        2008.
        \textit{Prepare for the Worst, Plan for the Best: Disaster Preparedness and Recovery for Small Businesses}.
	John Wiley \& Sons.
\bibitem{Cloud Security and Governance: Who's on your cloud?}
        Blount, S.,
	Zanella, R.
        2010.
	\textit{Cloud Security and Governance: Who's on your cloud?}
        IT Governance Publishing.

\end{thebibliography}

\end{document}
